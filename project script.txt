https://docs.google.com/presentation/d/1eETZHLye2pDxjCaqzUbXjjNR5laK7_5n/edit#slide=id.p16
4조 발표 시작하겠습니다.

네 안녕하세요 final 4조에 발표를 맡은 박재윤입니다. 잘부탁드립니다.(인사)
앞서 발표하기전에 말씀드릴게
저희가 진행한 프로젝트 이름은 피피티 도우미입니다. 
피피티 옆에 저희가 프로젝트로 구현한 클라이언트를 켜놨는데요,
피피티를 진행하면서 클라이언트로 계속해서 서비스를 시연하도록 하겠습니다.

네 저희 프로젝트이름은 말씀드렸다시피 ppt도우미입니다.
간단하게 목차부터 보여드리도록 하겠습니다.

'다음페이지'

팀소개 모델선정과정과 데이터전처리, 어플리케이션소개,QnA 마지막으로 느낀점을 말씀드리도록 하겠습니다

'다음페이지'

우선 간단하게 팀원소개부터 드리겠습니다.

프로젝트의 기획,계획조정,모델구현을 맡은 무준님,
그리고 서비스 구현을 맡으신 상택님,
그리고 자료조사와 모델 조정을 맡은 발표자 저, 박재윤 이렇게 세명이 구성되었습니다.
프로젝트의 진행은 자료에 맞는 데이터를 각자 수집해서 개인의 전략으로 전처리까지하고,
세명의 전처리방법으로 모델을 따로따로 훈련시킨뒤,
훈련된 모델의 품질이 가장 좋은 전처리를 채택하는식으로
팀원 모두가 모델훈련까지 참여하는 방식이었습니다.
그 이후의 구체적인 작업은 서로가 각자 맡은 역할을 수행하여
프로젝트를 진행했습니다.

'다음페이지'

그래서 우리가 무엇을 구현했느냐,
이렇게 제가 마이크로 명령음성을 입력을 하면은 
모델에서 음성를 텍스트로 바꿔주고,
이렇게 변환된 명령문의 텍스트를 
클라이언트에서 받아서 
내재되어있는 명령을 실행시켜주는 프로젝트입니다.
예를들어 시리나 빅스비가 피피티에 적용된것을 만드는게 저희 프로젝트인것이죠
여기서 음성을 텍스트로 바꿔줄때 네이버클로바같은 api를 사용할수 있었지만
오프라인환경에서도 사용할수 있게하기위해 저희가 모델을 직접 만들어 훈련시키기로 하였습니다
그래서 우선 입력된 음성명령을 텍스트로 반환해줄 스피치투텍스트, stt 모델이 필요했습니다.

'다음페이지'

모델은 wav2vec2 기반의 xlsr을 사용했습니다
모델선정 이유에대해서 말씀드리겠습니다

'다음페이지'

그전에 저희 자연어처리시간에는 텍스트를 학습시키는 방법을 배웠지 음성을 학습시키는 방법을 배우진않아서
음성데이터는 어떤식으로 모델에 학습시키는지 궁금하실분이 있을거라 생각합니다.
설명드리자면
다들 아시다시피 사실 디지털 신호는 우리가 실제 내는 소리의 근사치입니다.
우리 음성을 디지털 신호로 바꿀 때 우리가 낸 소리를 일정 간격으로 점을 찍어서 해당 점에 위치한 값을 기록하는 것이죠.
이 때 이 점을 찍어 기록하는것을 샘플링이라고 하고 우리가 흔히보는 wav파일 음성의 파동을 기록한 샘플링데이터를 가지고 있습니다 여기서 음성의 파동에 주목해주세요


'다음페이지'


사람은 음성을 어떻게 인식할까요
사람은 음성을 주파수로 인식합니다
간단하게 말씀드리면 소리의 주파수와 같은 주파수를 가진 청각세포가 공명하면서 뇌에 신호를 보냅니다
다시 돌아가서 wav파일은 음성의 파동의 값이 기록되어있습니다. 주파수와는 다르잖아요
그래서 모델들은 사람처럼 학습하기위해 파동을 주파수로 변환시켰고 이때 사용되는게 퓨리에 변환이죠

하지만 최근 음성처리 모델에서는 파동을 주파수로 변환시키지 않고
파동만을 가져와서 음성의 특징을 추출하고 학습하는 모델들이 나오고있어요
그 모델들이 저희가 사용할 모델이었습니다.

 
'다음페이지'

자료를 조사할때 stt모델 두개가 있었습니다.
deep speech2 모델기반의 kospeech이 있었구요 wav2vec2 기반의 xlsr 이 있었습니다.
모델선정과정에 기준은 이렇게 세가지가 있었는데 천천히 보자면
kospeech는 한국어학습이 되어있었지만 오류도 많고 사용하기 까다로웠습니다. 그리고 정확도가 높진 않았어요

xlsr모델은 한국어학습이 되어있지 않았지만 허깅페이스를 이용하여 쉽게 사용할수 있었구요, 정확도도 높았습니다.
그래서 xlsr로 가닥을 잡고 한국어 데이터의 샘플을 학습시켰는데
적은 데이터로도 좋은 성적이나왔습니다.
그리고 이 xlsr 모델은 예전에 nlp시간에 배웠던 word2vec모델과 비슷한 방식로 학습하기 때문에 이해하기도 쉬웠구요.
그래서 저희는 xlsr을 사용하기로 했습니다.


'다음페이지'

모델학습을 위해서 한국어음성 데이터는 ai허브에서 수집했습니다.
그리고 데이터들에는 wav혹은 pcm으로 된 오디오 파일과 오디오내용을 적은 text파일로 존재했습니다.

'다음페이지'

각자의 다른방식의 전처리를 위해 세가지의 데이터를 하나씩 맡아서 진행했습니다.
예를 들어 말중간중간에 소음부분,웃음소리,숨소리도 토큰처리를 
하는 전략으로요
하지만 결론적으로 말씀드리자면 그냥 단순히 단어들만 학습하는 게 나았습니다.

'다음페이지'

데이터 증강부분에선
강의실의 환경에서 잘 작동시키기위해 강의실의 공간감을 적용하여 데이터를 추가하였고
말의 속도에 상관없이 잘 작동하기위해 데이터의 재생속도를 빠르게하거나 느리게한 데이터를 추가하였습니다.

'다음페이지'

단어장은 구성할때
자모기반으로 구성하거나  음절기반으로 구성할수 있었는데
저희는 자모기반으로 단어장을 구성했습니다.
음절 기반을 사용했을 때 단어가 너무많아 많은 데이터를 사용하여도 비교적 적은 단어만 학습이 가능하였고
자모기반의 단어장은 크기가 작아 적은 학습으로도 대부분의 단어를 학습할수 있었습니다.
실제로 stt 성적도 자모기반이 좋았구요.

'다음페이지'

이렇게 모델학습은 끝냈고
이모델을 사용할수 있어야겠죠.
저희는 이모델을 이용하여 ppt를 도와주는 윈도우 어플리케이션 만들었습니다.

'다음페이지'

ui는 이렇게 구성되어있습니다.
(ppt읽기)
음성인식을 시작하기전에 이부분에서 마이크가 작동하는지 확인할수 있구요

'다음페이지'

이부분에서는 내재되어있는 명령어가 잘 동작하는지 테스트를 해볼수 있습니다

그리고 이버튼을 눌러서 음성인식을 시작하게 됩니다.

'다음페이지'

음성인식버튼을 누르게되면 여기서 상태메세지가 출력이되고
명령을 말씀해달라고 하면 명령을 한뒤 음성이 변환된 텍스트가 이부분에서 나오게 됩니다

'다음페이지'

추가로 어플리케이션의 성능을 높이기위해
다음과 같은 기술을 적용하였습니다
처음엔 패키지를 이용해서 음성감지를 해 음성신호를 모델로 보내주었지만
노이즈에 취약하여 음성을 너무 길게 인식하였습니다.
그래서 저희는 패키지를 이용하지 않고
대역필터를 적용하여 고주파 또는 저주파 노이즈를 음성으로 인식하지 않게 하였습니다

다음페이지

모델을 통해서 나온 텍스트 결과에 오차가 있더라도 
심스펠 알고리즘을 이용하여 단어오차를 줄여
알맞은 단어가 나오도록 서비스의 품질 높였습니다

'다음페이지'

이제 마무리를 할텐데 프로젝트를 진행하면서 느낀점들이 있었는데

(느낀점)

마무리하면서 qna를 진행하도록 하겟습니다

(qna)

'다음페이지'


이상으로 4조 프로젝트발표를 마치겠습니다
감사합니다.

'슬라이드쇼 종료'

