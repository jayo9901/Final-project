https://docs.google.com/presentation/d/1eETZHLye2pDxjCaqzUbXjjNR5laK7_5n/edit#slide=id.p16
4조 발표 시작하겠습니다.

'피피티 시작'

네 안녕하세요 final 4조에 발표를 맡은 박재윤입니다. 잘부탁드립니다.(인사)
앞서 슬라이드쇼를 시작하기전에 말씀드릴게
저희가 진행한 프로젝트 이름은 피피티 도우미입니다. 
피피티 옆에 저희가 프로젝트로 구현한 클라이언트를 켜놨는데요,
피피티를 진행하면서 클라이언트로 계속해서 서비스를 시연하도록 하겠습니다.

'슬라이드쇼 시작'

네 저희 프로젝트이름은 말씀드렸다시피 ppt도우미입니다.
간단하게 팀원 소개부터 드리겠습니다.

'다음페이지'
'다음페이지'

프로젝트의 기획,계획조정,모델구현을 맡은 무준님,
그리고 서비스 구현을 맡으신 상택님,
그리고 자료조사와 모델 조정을 맡은 발표자 저, 박재윤 이렇게 세명이 구성되었습니다.
프로젝트의 진행은 자료에 맞는 데이터를 각자 수집해서 개인의 전략으로 전처리까지하고,
세명의 전처리방법으로 모델을 따로따로 훈련시킨뒤,
훈련된 모델의 품질이 가장 좋은 전처리를 채택하는식으로
팀원 모두가 모델훈련까지 참여하는 방식이었습니다.
그 이후의 구체적인 작업은 서로가 각자 맡은 역할을 수행하여
프로젝트를 진행했습니다.

'다음페이지'

그래서 우리가 무엇을 구현했느냐,
이렇게 제가 마이크로 명령음성을 입력을 하면은 
모델에서 음성를 텍스트로 바꿔주고,
이렇게 변환된 명령문의 텍스트를 
클라이언트에서 받아서 
내재되어있는 명령을 실행시켜주는 프로젝트입니다.
예를들어 시리나 빅스비가 피피티에 적용된것을 만드는게 저희 프로젝트인것이죠
여기서 음성을 텍스트로 바꿔줄때 네이버클로바같은 api를 사용할수 있었지만
오프라인환경에서도 사용할수 있게하기위해 저희가 모델을 직접 만들어 훈련시키기로 하였습니다
그래서 우선 입력된 음성명령을 텍스트로 반환해줄 스피치투텍스트, stt 모델이 필요했습니다.

'다음페이지'

모델은 wav2vec2 기반의 xlsr을 사용했습니다
모델선정 이유에대해서 말씀드리겠습니다

'다음페이지'

그전에 저희 자연어처리시간에는 텍스트를 학습시키는 방법을 배웠지 음성을 학습시키는 방법을 배우진않아서
음성데이터는 어떤식으로 모델에 학습시키는지 궁금하실분이 있을거라 생각합니다.
설명드리자면
다들 아시다시피 사실 디지털 신호는 우리가 실제 내는 소리의 근사치입니다.
왜? 우리가 내는 소리는 일단 연속적이잖아요? 쪼갤수 없다는 말입니다.
우리 음성을 디지털 신호로 바꿀 때 어떻게 하냐면 우리가 낸 소리를 일정 간격으로 점을 찍어서 해당 점에 위치한 값을 기록하는 것이죠.
이 때 이 1초 동안 찍는 점의 갯수를 샘플 크기라고하고 우리가 흔히보는 wav파일은 1초당 16000개의 샘플크기를 가진 파일이에요.
그리고 wav파일은 음성의 파동을 기록한거에요.
여기서 음성의 파동에 주목해주세요

'다음페이지'

사람은 음성을 어떻게 인식할까요
간단하게 말하자면 소리의 주파수와 같은 주파수를 가진 청각세포가 공명하면서 뇌에 신호를 보내는데
여기서 소리의 주파수와 파동은 다른개념입니다.

다시 돌아가서 wav파일은 음성의 파동의 값이 기록되어있습니다. 주파수와는 다르죠?
파동이 아닌 주파수에는 음성들의 특징이 표현되어 있습니다.
그래서 모델들은 사람처럼 학습하기위해 파동을 주파수로 변환시키는게 필요했어요. 
이때 가끔씩 들어보셨을수도 있는 퓨리에 변환이란것을 사용하는거죠

하지만 최근 음성처리 모델에서는 파동을 주파수로 변환시키지 않고
파동만을 가져와서 음성의 특징을 추출하고 학습하는 모델들이 나오고있어요
그 모델들이 저희가 사용할 모델이었습니다.

 
'다음페이지'

자료를 조사할때 stt모델 두개가 있었습니다.
deep speech2 모델기반의 한국어학습이 된 kospeech와 
다국어로 사전학습된 wav2vec2 기반의 xlsr 
둘다 장단점이 있었어요.
kospeech는 한국어학습이 되어있었지만 오류도 많고 사용하기 까다로웠습니다.
xlsr모델은 허깅페이스를 이용하여 쉽게 사용할수있었지만 한국어학습이 되어있지 않았고요.
하지만 xlsr의 음성 모델부분이 잘 학습되어있어서 적은 한국어 데이터로도 좋은 성적이나왔습니다.
그리고 이 xlsr 모델은 예전에 nlp시간에 배웠던 word2vec모델과 비슷한 방식로 학습하기 때문에 이해하기도 쉬웠구요.
그래서 저희는 xlsr을 사용하기로 했습니다.



'다음페이지'

모델학습을 위해서 한국어음성 데이터는 ai허브에서 수집했습니다.
그리고 데이터들에는 wav혹은 pcm으로 된 오디오 파일과 오디오내용을 적은 text파일로 존재했습니다.

'다음페이지'

각자의 다른방식의 전처리를 위해 세가지의 데이터를 하나씩 맡아서 진행했습니다.
각자 다른방식이라면 예를 들어 소음부분도 학습시켜서 토큰처리를 한다던가
말 중간중간의 웃음소리,숨소리도 학습시켜서 토큰처리를 한다던가
그냥 다빼고 단어들만 학습하던가, 각자 다른 전략으로요.
결론적으로 말씀드리자면 그냥 다뺴고 단어들만 학습하는 전처리방식을 채택했습니다.

'다음페이지'

데이터 증강부분에선
강의실의 환경에서 잘 작동시키기위해 강의실의 공간감을 적용하여 데이터를 추가하였고
말의 속도에 상관없이 잘 작동하기위해 데이터의 재생속도를 빠르게하거나 느리게한 데이터를 추가하였습니다.

'다음페이지'

단어장은 구성할때
자모기반으로 구성하거나  낱말기반으로 구성할수 있었는데
저희는 자모기반으로 단어장을 구성했습니다.
낱말 기반을 사용했을 때 단어가 너무많아 많은 데이터를 사용하여도 비교적 적은 단어만 학습이 가능하였고
자모기반의 단어장은 크기가 작아 적은 학습으로도 대부분의 단어를 학습할수 있었습니다.
실제로 stt 성적도 자모기반이 좋았구요.

'다음페이지'

이렇게 모델학습은 끝냈고
이모델을 사용할수 있어야겠죠.
저희는 이모델을 이용하여 ppt를 도와주는 윈도우 어플리케이션 만들었습니다.

'다음페이지'

ui는 이렇게 구성되어있습니다.
(ppt읽기)
음성 인식 시작 버튼을 눌러 음성인식을 시작하고
테스트 시나리오대로 테스트 진행하며

'다음페이지'

음성 데이터가
오디오 디바이스에서 입력된 음성을 표시하게됩니다

'다음페이지'

이부분에서 음성을 텍스트로 표현해주고
음성을 어떻게 인식했는지 표시되는 부분입니다.

'다음페이지'

추가로 어플리케이션의 성능을 높이기위해
다음과 같은 기술을 적용하였습니다

'다음페이지'

처음엔 패키지를 이용해서 음성감지를 해 음성신호를 모델로 보내주었지만
노이즈에 취약하여 음성을 너무 길게 인식하였습니다.
그래서 저희는 패키지를 이용하지 않고
대역필터를 적용하여 고주파 또는 저주파 노이즈를 음성으로 인식하지 않게 하였습니다

(ppt)ai모델을 통해서 나온 텍스트 결과에 오차가 있더라도 
심스펠 알고리즘을 이용하여 단어오차를 줄여
알맞은 단어가 나오도록 서비스의 품질 높였습니다

'다음페이지'

이제 마무리를 할텐데 마무리 하기전에
질문있으신분은 질문받겠습니다.

(질문 받는 시간)

마무리하면서 프로젝트를 진행하면서 배우고 느낀점을 말씀드리자면

(느낀점)

'다음페이지'


이상으로 4조 프로젝트발표를 마치겠습니다
감사합니다.

'슬라이드쇼 종료'

